{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression  # Example model\n",
    "from sklearn.ensemble import RandomForestClassifier #Another Example Model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from imblearn.over_sampling import SMOTE  # Install: pip install imbalanced-learn\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "dataset = pd.read_csv('url_classification.csv')\n",
    "dataset.columns = ['sr_no', 'website_url', 'category']\n",
    "df = dataset.iloc[:, 1:].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available:  []\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs Available: \", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.18.0\n",
      "GPUs Available: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Computation Result: 32.0\n"
     ]
    }
   ],
   "source": [
    "# Force TensorFlow to use GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"GPU is now enabled for TensorFlow.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory growth: {e}\")\n",
    "\n",
    "# Run a sample computation\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0])\n",
    "    b = tf.constant([4.0, 5.0, 6.0])\n",
    "    result = tf.reduce_sum(a * b)\n",
    "    print(\"GPU Computation Result:\", result.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2568442150476920164\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     num_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(c\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m url)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, query_count, url_length, num_count\n\u001b[1;32m---> 19\u001b[0m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebsite_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwebsite_url\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebsite_url\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\victus123\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\victus123\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\victus123\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\apply.py:1514\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m-> 1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmapped\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor(mapped, index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   1517\u001b[0m         obj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1518\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\victus123\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m         arrays,\n\u001b[0;32m    861\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m     )\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\victus123\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\victus123\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py:839\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m--> 839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_list_of_series_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;66;03m# last ditch effort\u001b[39;00m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\victus123\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\internals\\construction.py:882\u001b[0m, in \u001b[0;36m_list_of_series_to_arrays\u001b[1;34m(data, columns)\u001b[0m\n\u001b[0;32m    880\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer_cache[\u001b[38;5;28mid\u001b[39m(index)]\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 882\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer_cache[\u001b[38;5;28mid\u001b[39m(index)] \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m values \u001b[38;5;241m=\u001b[39m extract_array(s, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    885\u001b[0m aligned_values\u001b[38;5;241m.\u001b[39mappend(algorithms\u001b[38;5;241m.\u001b[39mtake_nd(values, indexer))\n",
      "File \u001b[1;32mc:\\Users\\victus123\\anaconda3\\envs\\my_conda_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3940\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pself\u001b[38;5;241m.\u001b[39mget_indexer(\n\u001b[0;32m   3935\u001b[0m         ptarget, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[0;32m   3936\u001b[0m     )\n\u001b[0;32m   3938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m target\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mequals(target):\n\u001b[0;32m   3939\u001b[0m     \u001b[38;5;66;03m# Only call equals if we have same dtype to avoid inference/casting\u001b[39;00m\n\u001b[1;32m-> 3940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m target\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_partial_index(target):\n\u001b[0;32m   3943\u001b[0m     \u001b[38;5;66;03m# _should_partial_index e.g. IntervalIndex with numeric scalars\u001b[39;00m\n\u001b[0;32m   3944\u001b[0m     \u001b[38;5;66;03m#  that can be matched to Interval scalars.\u001b[39;00m\n\u001b[0;32m   3945\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_common_type_compat(target)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_url(url):\n",
    "    url = re.sub(r\"https?://\", \"\", url)\n",
    "    url = re.sub(r\"www\\d?\\.\", \"\", url)\n",
    "    url = re.sub(r\"[^\\w\\s]\", \" \", url)\n",
    "    url = re.sub(r\"\\s+\", \" \", url).strip()\n",
    "    return url\n",
    "\n",
    "df['website_url'] = df['website_url'].apply(preprocess_url)\n",
    "\n",
    "def extract_features(url):\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.replace(\"www.\", \"\")\n",
    "    path = parsed.path.replace(\"/\", \"\")\n",
    "    query_count = len(parsed.query.split('&')) if parsed.query else 0\n",
    "    url_length = len(url)\n",
    "    num_count = sum(c.isdigit() for c in url)\n",
    "    return f\"{domain} {path}\", query_count, url_length, num_count\n",
    "\n",
    "df[[\"website_url\", \"query_count\", \"url_length\", \"num_count\"]] = df['website_url'].apply(lambda x: pd.Series(extract_features(x)))\n",
    "\n",
    "X = df['website_url']\n",
    "y = df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Feature Engineering Pipeline\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemmed_tokenizer(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 3), tokenizer=stemmed_tokenizer)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('select', SelectKBest(chi2, k=5000))  # Adjust k as needed\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', text_pipeline, 'website_url'),\n",
    "    ('num', num_pipeline, ['query_count', 'url_length', 'num_count'])\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# Apply SMOTE *after* transformation\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "# Model Training and Evaluation (Logistic Regression Example)\n",
    "pipeline = Pipeline([\n",
    "    ('clf', LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)) # solver for l1 penalty\n",
    "])\n",
    "\n",
    "# Grid Search (Optional - uncomment to use)\n",
    "# param_grid = {\n",
    "#     'clf__C': [0.1, 1, 10],\n",
    "#     'clf__penalty': ['l1', 'l2']\n",
    "# }\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs = -1)\n",
    "# grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = pipeline.predict(X_test_transformed)\n",
    "\n",
    "print(\"\\n✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n📊 Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=1))  # Handle zero division\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# New URL Predictions\n",
    "new_urls = [\n",
    "    \"https://www.espn.com/nba/scores\",\n",
    "    \"http://netflix.com/show/98765\",\n",
    "    \"https://github.com/tensorflow/models\",\n",
    "    \"https://www.bbc.com/news/politics\",\n",
    "    \"renewable-resources.netlify.app\"\n",
    "]\n",
    "\n",
    "new_urls_processed = [preprocess_url(url) for url in new_urls]\n",
    "new_df = pd.DataFrame({\"website_url\": new_urls_processed, \"query_count\": [0]*len(new_urls), \"url_length\": [len(url) for url in new_urls], \"num_count\": [sum(c.isdigit() for c in url) for url in new_urls]})\n",
    "new_urls_transformed = preprocessor.transform(new_df)\n",
    "new_predictions = pipeline.predict(new_urls_transformed)\n",
    "\n",
    "print(\"\\n🔹 Predictions on New URLs:\")\n",
    "for url, category in zip(new_urls, new_predictions):\n",
    "    print(f\"{url} → {category}\")\n",
    "\n",
    "\n",
    "#Random Forest Classifier\n",
    "pipeline_rf = Pipeline([\n",
    "    ('clf', RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_rf.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_rf = pipeline_rf.predict(X_test_transformed)\n",
    "\n",
    "print(\"\\n✅ Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\n📊 Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf, zero_division=1))  # Handle zero division\n",
    "print(\"\\nRandom Forest Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "\n",
    "new_predictions_rf = pipeline_rf.predict(new_urls_transformed)\n",
    "\n",
    "print(\"\\n🔹 Random Forest Predictions on New URLs:\")\n",
    "for url, category in zip(new_urls, new_predictions_rf):\n",
    "    print(f\"{url} → {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('url_classification.csv')\n",
    "dataset.columns = ['sr_no', 'website_url', 'category']\n",
    "df = dataset.iloc[:, 1:]\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "# Preprocess URLs\n",
    "def preprocess_url(url):\n",
    "    url = re.sub(r\"https?://\", \"\", url)        # Remove protocols\n",
    "    url = re.sub(r\"www\\d?\\.\", \"\", url)         # Remove www\n",
    "    url = re.sub(r\"[^\\w\\s]\", \" \", url)         # Remove special characters\n",
    "    url = re.sub(r\"\\s+\", \" \", url).strip()     # Remove extra spaces\n",
    "    return url\n",
    "\n",
    "df['website_url'] = df['website_url'].apply(preprocess_url)\n",
    "\n",
    "# Extract features\n",
    "def extract_features(url):\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.replace(\"www.\", \"\")\n",
    "    path = parsed.path.replace(\"/\", \"\")\n",
    "    query_count = len(parsed.query.split('&')) if parsed.query else 0\n",
    "    url_length = len(url)\n",
    "    num_count = sum(c.isdigit() for c in url)\n",
    "    return f\"{domain} {path}\", query_count, url_length, num_count\n",
    "\n",
    "df[[\"website_url\", \"query_count\", \"url_length\", \"num_count\"]] = df['website_url'].apply(lambda x: pd.Series(extract_features(x)))\n",
    "\n",
    "# Split into training and test data\n",
    "test_samples_per_class = 2000\n",
    "test_dfs = []\n",
    "\n",
    "for category in df[\"category\"].unique():\n",
    "    test_dfs.append(df[df[\"category\"] == category].sample(test_samples_per_class, random_state=42))\n",
    "\n",
    "test_data = pd.concat(test_dfs)\n",
    "train_data = df.drop(test_data.index)\n",
    "\n",
    "X_train = train_data.drop(columns=['category'])\n",
    "y_train = train_data['category']\n",
    "X_test = test_data.drop(columns=['category'])\n",
    "y_test = test_data['category']\n",
    "\n",
    "# Compute class weights based on frequency\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# Resampling (Undersampling majority classes)\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Custom tokenizer with SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemmed_tokenizer(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)  # Remove special characters\n",
    "    tokens = text.split()\n",
    "    return [stemmer.stem(token) for token in tokens]  # Apply stemming\n",
    "\n",
    "# Define transformation pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(stop_words=None, ngram_range=(1, 3), tokenizer=stemmed_tokenizer)),  # Use 1-3 grams\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', text_pipeline, 'website_url'),\n",
    "    ('num', num_pipeline, ['query_count', 'url_length', 'num_count'])\n",
    "])\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_train)), \n",
    "                          eval_metric='mlogloss', use_label_encoder=False, \n",
    "                          scale_pos_weight=class_weight_dict)\n",
    "\n",
    "# Final Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('features', preprocessor),\n",
    "    ('clf', xgb_model)\n",
    "])\n",
    "\n",
    "# Train the Model\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n📊 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(12,6))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# 🔹 **Test New URLs**\n",
    "new_urls = [\n",
    "    \"https://www.espn.com/nba/scores\",\n",
    "    \"http://netflix.com/show/98765\",\n",
    "    \"https://github.com/tensorflow/models\",\n",
    "    \"https://www.bbc.com/news/politics\",\n",
    "    \"renewable-resources.netlify.app\"\n",
    "]\n",
    "\n",
    "new_urls_processed = [preprocess_url(url) for url in new_urls]\n",
    "predictions = pipeline.predict(pd.DataFrame({\n",
    "    \"website_url\": new_urls_processed,\n",
    "    \"query_count\": [0]*len(new_urls),\n",
    "    \"url_length\": [len(url) for url in new_urls],\n",
    "    \"num_count\": [sum(c.isdigit() for c in url) for url in new_urls]\n",
    "}))\n",
    "\n",
    "print(\"\\n🔹 Predictions on New URLs:\")\n",
    "for url, category in zip(new_urls, predictions):\n",
    "    print(f\"{url} → {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
